{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from rouge_score import rouge_scorer\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "import warnings\n",
    "\n",
    "# Suppress UserWarning from transformers related to Flash Attention\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers.models.bert.modeling_bert\")\n",
    "\n",
    "# Constants\n",
    "COMPANY_DATA = './data/companies_data.csv'\n",
    "REFERENCE_DATA = './data/references.csv'\n",
    "VECTOR_SIZE = 1024\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 200\n",
    "LLM_PRETRAINED_MODEL = \"llama3.1\"\n",
    "MODEL_NAME = \"BAAI/bge-large-en\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "COLLECTION_NAME = \"vector_db\"\n",
    "def pre_process(df):\n",
    "    cols = df.columns\n",
    "    first_col = df[cols[0]].tolist()\n",
    "    second_col = df[cols[1]].tolist()\n",
    "    first_col = [f.translate(str.maketrans('', '', string.punctuation)) for f in first_col]\n",
    "    second_col = [s.translate(str.maketrans('', '', string.punctuation)) for s in second_col]\n",
    "    df_pre_processed = pd.DataFrame({cols[0]: first_col, cols[1]: second_col})\n",
    "    return df_pre_processed\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def evaluate_result(result, ground_truth):\n",
    "    scores = scorer.score(ground_truth, result)\n",
    "    rouge1 = scores['rouge1'].fmeasure\n",
    "    rouge2 = scores['rouge2'].fmeasure\n",
    "    rougel = scores['rougeL'].fmeasure\n",
    "    return rouge1, rouge2, rougel\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "\n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "# Load and pre-process reference data\n",
    "df_references = pre_process(pd.read_csv(REFERENCE_DATA))\n",
    "questions = df_references['question']\n",
    "answers = df_references['expected_answer']\n",
    "\n",
    "# Load and pre-process companies data\n",
    "df = pre_process(pd.read_csv(COMPANY_DATA))\n",
    "df = df.drop_duplicates(subset=['companyName'])\n",
    "company_names = df['companyName'].tolist()\n",
    "descriptions = df['description'].tolist()\n",
    "\n",
    "# Tokenize the text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "company_name_tokens = [text_splitter.split_text('companyName: '+name) for name in company_names]\n",
    "description_tokens = [text_splitter.split_text('description: '+desc) for desc in descriptions]\n",
    "\n",
    "# Create the corpus\n",
    "corpus = [Document(page_content=\"\\n\".join(name + desc), metadata={\"id\": i}) for i, (name, desc) in enumerate(zip(company_name_tokens, description_tokens))]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(corpus)\n",
    "\n",
    "# Pre-trained LLM (LLAMA 3.1, 8B parameters)\n",
    "llm = Ollama(model=LLM_PRETRAINED_MODEL)\n",
    "\n",
    "# Initialize embedding Model\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Initialize Qdrant client and create collection if it doesn't exist\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL, prefer_grpc=False\n",
    ")\n",
    "\n",
    "if not client.collection_exists(COLLECTION_NAME):\n",
    "    # Create the collection if it doesn't exist\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=VECTOR_SIZE,  # Adjust according to your needs\n",
    "            distance=models.Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Store the database\n",
    "db = Qdrant.from_documents(\n",
    "    splits,\n",
    "    embeddings,\n",
    "    url=QDRANT_URL,\n",
    "    prefer_grpc=False,\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "\n",
    "# Retriever with top-k=2\n",
    "retriever = db.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "# Multi query: different perspectives\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Chain of multi-query generation and filter\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "# Initialize rouge scorer and cosine-similarity list\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougel_scores = []\n",
    "\n",
    "cosine_similarities = []\n",
    "\n",
    "# Retrieve multi-query per question, filtering, and Evaluation\n",
    "for idx, question in enumerate(questions):\n",
    "    print(f\"Question {idx} :\")\n",
    "    print(question + \"?\")\n",
    "    print(\"GROUND TRUTH:\")\n",
    "    ground_truth = answers[idx]\n",
    "    print(ground_truth)\n",
    "\n",
    "    # Retrieve multi-query per question\n",
    "    docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "    # Main Prompt Template\n",
    "    template = \"\"\"You are an AI assistant that you know some companies and their description. Consider companyName and its description as a pair. Answer the question in a single sentence. If you don't know the answer, just say \"I don't know\" without elaborating further. Use the following pieces of context to answer the question at the end.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    final_rag_chain = (\n",
    "        {\"context\": retrieval_chain_rag_fusion,\n",
    "        \"question\": itemgetter(\"question\")}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    print(\"RAG Fusion RESULT:\")\n",
    "    result = final_rag_chain.invoke({\"question\":question})\n",
    "    print(result)\n",
    "    print('-'.join('' for x in range(100)))\n",
    "\n",
    "    rouge1, rouge2, rougel = evaluate_result(result, ground_truth)\n",
    "    rouge1_scores.append(rouge1)\n",
    "    rouge2_scores.append(rouge2)\n",
    "    rougel_scores.append(rougel)\n",
    "\n",
    "    result_embedding = embeddings.embed_query(result)\n",
    "    ground_truth_embedding = embeddings.embed_query(ground_truth)\n",
    "    cosine_sim = cosine_similarity(result_embedding, ground_truth_embedding)\n",
    "    cosine_similarities.append(cosine_sim)\n",
    "\n",
    "print(\"Evaluation:\")\n",
    "print(f\"ROUGE-1: {np.mean(rouge1_scores):.3f}\")\n",
    "print(f\"ROUGE-2: {np.mean(rouge2_scores):.3f}\")\n",
    "print(f\"ROUGE-L: {np.mean(rougel_scores):.3f}\")\n",
    "\n",
    "print('.'.join('' for x in range(100)))\n",
    "print(f\"COSINE_SIMILARITY: {np.mean(cosine_similarities):.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from qdrant_client.http import models\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from rouge_score import rouge_scorer\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "import warnings\n",
    "\n",
    "# Suppress UserWarning from transformers related to Flash Attention\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers.models.bert.modeling_bert\")\n",
    "\n",
    "# Constants\n",
    "COMPANY_DATA = './data/companies_data.csv'\n",
    "REFERENCE_DATA = './data/references.csv'\n",
    "VECTOR_SIZE = 1024\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 200\n",
    "LLM_PRETRAINED_MODEL = \"llama3.1\"\n",
    "MODEL_NAME = \"BAAI/bge-large-en\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "COLLECTION_NAME = \"vector_db\"\n",
    "def pre_process(df):\n",
    "    cols = df.columns\n",
    "    first_col = df[cols[0]].tolist()\n",
    "    second_col = df[cols[1]].tolist()\n",
    "    first_col = [f.translate(str.maketrans('', '', string.punctuation)) for f in first_col]\n",
    "    second_col = [s.translate(str.maketrans('', '', string.punctuation)) for s in second_col]\n",
    "    df_pre_processed = pd.DataFrame({cols[0]: first_col, cols[1]: second_col})\n",
    "    return df_pre_processed\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def evaluate_result(result, ground_truth):\n",
    "    scores = scorer.score(ground_truth, result)\n",
    "    rouge1 = scores['rouge1'].fmeasure\n",
    "    rouge2 = scores['rouge2'].fmeasure\n",
    "    rougel = scores['rougeL'].fmeasure\n",
    "    return rouge1, rouge2, rougel\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Load and pre-process reference data\n",
    "df_references = pre_process(pd.read_csv(REFERENCE_DATA))\n",
    "questions = df_references['question']\n",
    "answers = df_references['expected_answer']\n",
    "\n",
    "# Load and pre-process companies data\n",
    "df = pre_process(pd.read_csv(COMPANY_DATA))\n",
    "df = df.drop_duplicates(subset=['companyName'])\n",
    "company_names = df['companyName'].tolist()\n",
    "descriptions = df['description'].tolist()\n",
    "\n",
    "# Tokenize the text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "company_name_tokens = [text_splitter.split_text('companyName: '+name) for name in company_names]\n",
    "description_tokens = [text_splitter.split_text('description: '+desc) for desc in descriptions]\n",
    "\n",
    "# Create the corpus\n",
    "corpus = [Document(page_content=\"\\n\".join(name + desc), metadata={\"id\": i}) for i, (name, desc) in enumerate(zip(company_name_tokens, description_tokens))]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "splits = text_splitter.split_documents(corpus)\n",
    "\n",
    "# Pre-trained LLM (LLAMA 3.1, 8B parameters)\n",
    "llm = Ollama(model=LLM_PRETRAINED_MODEL)\n",
    "\n",
    "# Initialize embedding model\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Initialize Qdrant client and create collection if it doesn't exist\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL, prefer_grpc=False\n",
    ")\n",
    "\n",
    "if not client.collection_exists(COLLECTION_NAME):\n",
    "    # Create the collection if it doesn't exist\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=VECTOR_SIZE,  # Adjust according to your needs\n",
    "            distance=models.Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Store the database\n",
    "db = Qdrant.from_documents(\n",
    "    splits,\n",
    "    embeddings,\n",
    "    url=QDRANT_URL,\n",
    "    prefer_grpc=False,\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "\n",
    "# Retriever with top-k=2\n",
    "retriever = db.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "# Multi-query: different perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate three \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "# Chain of multi-query generation and get union of docs\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "# Initialize rouge scorer and cosine-similarity list\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougel_scores = []\n",
    "\n",
    "cosine_similarities = []\n",
    "\n",
    "# Retrieve multi-query per question and evaluation\n",
    "for idx, question in enumerate(questions):\n",
    "    print(f\"Question {idx} :\")\n",
    "    print(question + \"?\")\n",
    "    print(\"GROUND TRUTH:\")\n",
    "    ground_truth = answers[idx]\n",
    "    print(ground_truth)\n",
    "\n",
    "    # Retrieve multi-query per question\n",
    "    docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "    # Main prompt template\n",
    "    template = \"\"\"You are an AI assistant that you know some companies and their description. Consider companyName and its description as a pair to answer the question. Some questions give you some information about a company and ask you its name, some questions ask some details about a company name. Answer the question in a single sentence. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that I don't know, don't try to make up an answer.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    final_rag_chain = (\n",
    "        {\"context\": retrieval_chain,\n",
    "        \"question\": itemgetter(\"question\")}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    print(\"RAG Multi-Query RESULT:\")\n",
    "    result = final_rag_chain.invoke({\"question\":question})\n",
    "    print(result)\n",
    "    print('-'.join('' for x in range(100)))\n",
    "\n",
    "    rouge1, rouge2, rougel = evaluate_result(result, ground_truth)\n",
    "    rouge1_scores.append(rouge1)\n",
    "    rouge2_scores.append(rouge2)\n",
    "    rougel_scores.append(rougel)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    result_embedding = embeddings.embed_query(result)\n",
    "    ground_truth_embedding = embeddings.embed_query(ground_truth)\n",
    "    cosine_sim = cosine_similarity(result_embedding, ground_truth_embedding)\n",
    "    cosine_similarities.append(cosine_sim)\n",
    "\n",
    "print(\"Evaluation: \")\n",
    "print(f\"ROUGE-1: {np.mean(rouge1_scores):.3f}\")\n",
    "print(f\"ROUGE-2: {np.mean(rouge2_scores):.3f}\")\n",
    "print(f\"ROUGE-L: {np.mean(rougel_scores):.3f}\")\n",
    "\n",
    "print('.'.join('' for x in range(100)))\n",
    "print(f\"COSINE_SIMILARITY: {np.mean(cosine_similarities):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
